import os
import json
import csv
import pandas as pd

folder_path = './'
output_csv = 'merged_output.csv'
base_file = 'xxx.xls'
base_fields = []
is_txt_header = True  # æ·»åŠ å¼€å…³ï¼Œé»˜è®¤ä¸º True è¡¨ç¤ºç¬¬ä¸€è¡Œä¸ºæ ‡é¢˜
all_rows = []  # å­˜å‚¨æ‰€æœ‰æ–‡ä»¶çš„æ•°æ®è¡Œ


def print_logo():
    logo = '''
    _____.___..___ _____.___..___        
    \__  |   ||   |\__  |   ||   |       
     /   |   ||   | /   |   ||   |       
     \____   ||   | \____   ||   |       
     / ______||___| / ______||___| ______
     \/             \/            /_____/
     '''
    print(logo)


def get_base_fields():
    global base_fields
    if base_file.endswith('.csv'):
        try:
            with open(base_file, 'r', encoding='utf-8-sig') as infile:
                reader = csv.DictReader(infile)
                base_fields = reader.fieldnames
        except Exception as e:
            print(f"è¯»å–åŸºç¡€æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    elif base_file.endswith('.json'):
        try:
            with open(base_file, 'r', encoding='utf-8') as infile:
                data = json.load(infile)
                if isinstance(data, list):
                    base_fields = list(data[0].keys())
                elif isinstance(data, dict):
                    base_fields = list(data.keys())
        except Exception as e:
            print(f"è¯»å–åŸºç¡€æ–‡ä»¶æ—¶å‡ºé”™: {e}")


def check_additional_fields(current_fields):
    global base_fields
    if current_fields is None:
        return
    additional_fields = [field for field in current_fields if field not in base_fields]
    if additional_fields:
        print(f'ğŸ¤£å‘ç°æ–°å­—æ®µ{additional_fields}')
        include_fields = input(f"ğŸ˜Šæ˜¯å¦å°†å­—æ®µ [{', '.join(additional_fields)}] åŒ…å«åˆ°CSVä¸­ï¼Ÿ(y/n): ").lower()
        if include_fields == 'y':
            base_fields.extend(additional_fields)


def process_files():
    global base_fields, all_rows

    file_types = ['.csv', '.xlsx', '.json', '.sql', '.txt', '.xls', '.xlsm']

    for file_type in file_types:
        print(f"ğŸ’¥æ­£åœ¨æå– {file_type} æ–‡ä»¶...")
        for filename in sorted(os.listdir(folder_path)):
            print(f'å½“å‰æå–æ–‡ä»¶{filename}...ğŸ’Ÿ')
            if filename.endswith(file_type):
                file_path = os.path.join(folder_path, filename)

                if file_type == '.csv':
                    with open(file_path, 'r', encoding='utf-8-sig') as infile:
                        reader = csv.DictReader(infile)
                        check_additional_fields(reader.fieldnames)
                        for row in reader:
                            all_rows.append([row.get(field, '') for field in base_fields])
                    print(f"CSV æ–‡ä»¶ {filename} æå–å®Œæ¯•")

                elif file_type in ['.xlsx', '.xls', '.xlsm']:
                    try:
                        df = pd.read_excel(file_path)
                        current_fields = df.columns.tolist()
                        check_additional_fields(current_fields)
                        for index, row in df.iterrows():
                            all_rows.append([row.get(field, '') for field in base_fields])
                    except Exception as e:
                        print(f"è¯»å–Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}")
                    print(f"Excel æ–‡ä»¶ {filename} æå–å®Œæ¯•")

                elif file_type == '.json':
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        data = json.load(infile)
                        if isinstance(data, list):
                            check_additional_fields(data[0].keys())
                            for entry in data:
                                all_rows.append([entry.get(field, '') for field in base_fields])
                        elif isinstance(data, dict):
                            check_additional_fields(data.keys())
                            all_rows.append([data.get(field, '') for field in base_fields])
                    print(f"JSON æ–‡ä»¶ {filename} æå–å®Œæ¯•")

                elif file_type == '.sql':
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        sql_lines = infile.readlines()
                        for line in sql_lines:
                            if line.strip().lower().startswith("insert into"):
                                values_part = line.split("VALUES")[1].strip().strip("();")
                                values = [v.strip().strip("'") for v in values_part.split(',')]
                                all_rows.append(
                                    [values[i] if i < len(values) else '' for i in range(len(base_fields))])
                    print(f"SQL æ–‡ä»¶ {filename} æå–å®Œæ¯•")

                elif file_type == '.txt':
                    with open(file_path, 'r', encoding='utf-8') as infile:
                        lines = infile.readlines()
                        header_handled = False
                        for row in lines:
                            fields = [item.strip() for item in row.strip().split('\t')]
                            if not header_handled and is_txt_header:
                                if base_fields == []:
                                    base_fields = fields
                                else:
                                    check_additional_fields(fields)
                                header_handled = True
                            else:
                                all_rows.append(fields)
                    print(f"TXT æ–‡ä»¶ {filename} æå–å®Œæ¯•")


def write_to_csv():
    global base_fields, all_rows
    with open(output_csv, 'w', newline='', encoding='utf-8-sig') as csvfile:
        csv_writer = csv.writer(csvfile)
        csv_writer.writerow(base_fields)  # å†™å…¥å¤´éƒ¨
        csv_writer.writerows(all_rows)  # å†™å…¥æ‰€æœ‰è¡Œæ•°æ®


if __name__ == '__main__':
    print_logo()
    get_base_fields()
    process_files()
    write_to_csv()
    print('Done~ğŸ˜')

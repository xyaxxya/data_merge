import os
import json
import csv
import pandas as pd

folder_path = './'
output_csv = 'xxxx.csv'
is_txt_header = True  # é»˜è®¤ä¸º True è¡¨ç¤ºç¬¬ä¸€è¡Œä¸ºæ ‡é¢˜
all_rows = []  # å­˜å‚¨æ‰€æœ‰æ–‡ä»¶çš„æ•°æ®è¡Œ
base_fields = []  # å­˜å‚¨åŸºç¡€å­—æ®µ
base_file = None  # å­˜å‚¨åŸºç¡€æ–‡ä»¶å
processed_files = set()  # ç”¨äºå­˜å‚¨å·²å¤„ç†æ–‡ä»¶çš„é›†åˆ
not_extracted_files = []  # ç”¨äºè®°å½•æœªæå–çš„æ–‡ä»¶

def print_logo():
    logo = '''
    _____.___..___ _____.___..___        
    \__  |   ||   |\__  |   ||   |       
     /   |   ||   | /   |   ||   |       
     \____   ||   | \____   ||   |       
     / ______||___| / ______||___| ______
     \/             \/            /_____/
     '''
    print(logo)

def get_base_fields():
    global base_fields, base_file
    files = [f for f in os.listdir(folder_path) if f.endswith(('.csv', '.json', '.xlsx', '.xls', '.sql', '.txt'))]

    if not files:
        print("æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ”¯æŒçš„æ–‡ä»¶ã€‚")
        return

    base_file = files[0]

    if base_file.endswith('.csv'):
        try:
            with open(os.path.join(folder_path, base_file), 'r', encoding='utf-8-sig') as infile:
                reader = csv.DictReader(infile)
                base_fields = reader.fieldnames
        except Exception as e:
            print(f"è¯»å–åŸºç¡€æ–‡ä»¶æ—¶å‡ºé”™: {e}")
    elif base_file.endswith('.json'):
        try:
            with open(os.path.join(folder_path, base_file), 'r', encoding='utf-8') as infile:
                data = json.load(infile)
                if isinstance(data, list):
                    base_fields = list(data[0].keys())
                elif isinstance(data, dict):
                    base_fields = list(data.keys())
        except Exception as e:
            print(f"è¯»å–åŸºç¡€æ–‡ä»¶æ—¶å‡ºé”™: {e}")

def check_additional_fields(current_fields):
    global base_fields
    if current_fields is None:
        return
    additional_fields = [field for field in current_fields if field not in base_fields]
    if additional_fields:
        print(f'ğŸ¤£å‘ç°æ–°å­—æ®µ{additional_fields}')
        include_fields = input(f"ğŸ˜Šæ˜¯å¦å°†å­—æ®µ [{', '.join(additional_fields)}] åŒ…å«åˆ°CSVä¸­ï¼Ÿ(y/n): ").lower()
        if include_fields == 'y':
            base_fields.extend(additional_fields)

def process_files():
    global base_fields, all_rows, base_file, processed_files, not_extracted_files

    file_types = ['.csv', '.xlsx', '.json', '.sql', '.txt', '.xls', '.xlsm']

    base_file_path = os.path.join(folder_path, base_file)

    for file_type in file_types:
        print(f"ğŸ’¥æ­£åœ¨æå– {file_type} æ–‡ä»¶...")
        for filename in sorted(os.listdir(folder_path)):
            current_file_path = os.path.join(folder_path, filename)
            if current_file_path in processed_files or current_file_path == base_file_path:
                continue  # è·³è¿‡åŸºç¡€æ–‡ä»¶å’Œå·²å¤„ç†çš„æ–‡ä»¶

            if filename.endswith(file_type):
                processed_files.add(current_file_path)
                print(f'å½“å‰æå–æ–‡ä»¶{filename}...ğŸ’Ÿ')
                if file_type == '.csv':
                    with open(current_file_path, 'r', encoding='utf-8-sig') as infile:
                        reader = csv.DictReader(infile)
                        check_additional_fields(reader.fieldnames)
                        for row in reader:
                            all_rows.append([row.get(field, '') for field in base_fields])
                    print(f"CSV æ–‡ä»¶ {filename} æå–å®Œæ¯•")
                elif file_type in ['.xlsx', '.xls', '.xlsm']:
                    try:
                        df = pd.read_excel(current_file_path)
                        current_fields = df.columns.tolist()
                        check_additional_fields(current_fields)
                        for index, row in df.iterrows():
                            all_rows.append([row.get(field, '') for field in base_fields])
                    except Exception as e:
                        print(f"è¯»å–Excelæ–‡ä»¶æ—¶å‡ºé”™: {e}")
                    print(f"Excel æ–‡ä»¶ {filename} æå–å®Œæ¯•")
                elif file_type == '.json':
                    with open(current_file_path, 'r', encoding='utf-8') as infile:
                        data = json.load(infile)
                        if isinstance(data, list):
                            check_additional_fields(data[0].keys())
                            for entry in data:
                                all_rows.append([entry.get(field, '') for field in base_fields])
                        elif isinstance(data, dict):
                            check_additional_fields(data.keys())
                            all_rows.append([data.get(field, '') for field in base_fields])
                    print(f"JSON æ–‡ä»¶ {filename} æå–å®Œæ¯•")
                elif file_type == '.sql':
                    with open(current_file_path, 'r', encoding='utf-8') as infile:
                        sql_lines = infile.readlines()
                        for line in sql_lines:
                            if line.strip().lower().startswith("insert into"):
                                values_part = line.split("VALUES")[1].strip().strip("();")
                                values = [v.strip().strip("'") for v in values_part.split(',')]
                                all_rows.append(
                                    [values[i] if i < len(values) else '' for i in range(len(base_fields))])
                    print(f"SQL æ–‡ä»¶ {filename} æå–å®Œæ¯•")
                elif file_type == '.txt':
                    with open(current_file_path, 'r', encoding='utf-8') as infile:
                        lines = infile.readlines()
                        header_handled = False
                        for row in lines:
                            fields = [item.strip() for item in row.strip().split('\t')]
                            if not header_handled and is_txt_header:
                                if base_fields == []:
                                    base_fields = fields
                                else:
                                    check_additional_fields(fields)
                                header_handled = True
                            else:
                                all_rows.append(fields)
                    print(f"TXT æ–‡ä»¶ {filename} æå–å®Œæ¯•")
            else:
                not_extracted_files.append(filename)

def write_to_csv():
    global base_fields, all_rows
    with open(output_csv, 'w', newline='', encoding='utf-8-sig') as csvfile:
        csv_writer = csv.writer(csvfile)
        csv_writer.writerow(base_fields)
        csv_writer.writerows(all_rows)

if __name__ == '__main__':
    print_logo()
    get_base_fields()
    process_files()
    write_to_csv()
    print('Done~ğŸ˜')
    print("æœªæå–çš„æ–‡ä»¶âŒï¼š", list(set(not_extracted_files)))

